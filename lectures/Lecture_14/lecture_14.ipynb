{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #14: Neural Network Models for Regression\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "### Import basic libraries\n",
    "from autograd import numpy as np\n",
    "from autograd import grad\n",
    "import numpy\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import rc\n",
    "from IPython.display import HTML\n",
    "from IPython.display import YouTubeVideo\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Administrative Matters\n",
    "\n",
    "1. **Attendance Quiz:**<br><br>\n",
    "2. **Projects:** Read Check Point #2 carefully -- **you** are responsible for setting up meetings with your TF and instructors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "1. Regression as Generalized Linear Models\n",
    "2. Neural Networks\n",
    "3. Automatic Differentiation and BackPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression as Generalized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression Models\n",
    "Here is a generalized linear model (GLM) you've known since day one!\n",
    "\\begin{align}\n",
    "\\mu &= \\mathbf{w}^\\top \\mathbf{X}^{(n)}\\\\\n",
    "Y^{(n)}&\\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "\\end{align}\n",
    "Alternatively, we can write this model as\n",
    "\n",
    "\\begin{align}\n",
    "    Y^{(n)} = \\mathbf{w}^\\top \\mathbf{X}^{(n)} + \\epsilon; \\quad \\epsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "\\end{align}\n",
    "\n",
    "That is, injecting covariates into a normal likelihood is precisely linear regression! Just like in the case of logistic regression, we can form scientific hypotheses by examining the parameters of a linear regression model:\n",
    "\n",
    "\\begin{align}\n",
    "    \\widehat{\\text{income}} = 2 * \\text{education (yr)} + 3.1 * \\text{married} - 1.5 * \\text{gaps in work history}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Would You Parameterize a Non-linear Trend?\n",
    "<img src=\"./fig/fig12.png\" style='height:400px;'>\n",
    "It's not easy to think of a function $g(x)$ can capture the trend in the data, e.g. what degree of polynomial should we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review of the Geometry of Logistic Regression \n",
    "In **logistic regression**, we model the probability of an input $\\mathbf{x}$ being labeled '1' as a function of its distance from the hyperplane parametrized by $\\mathbf{w}$\n",
    "<img src=\"./fig/fig0.png\" style='height:300px;'>\n",
    "That is, we model $p(y=1 | \\mathbf{w}, \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x})$. Where $\\mathbf{w}^\\top \\mathbf{x}=0$ is the equation of the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize a ellipitical decision boundary?\n",
    "\n",
    "<img src=\"./fig/fig1.png\" style='height:300px;'>\n",
    "\n",
    "We can say that the decision boundary is given by a ***quadratic function*** of the input:\n",
    "$$\n",
    "w_1x^2_1 + w_2x^2_2 + w_3 = 0\n",
    "$$\n",
    "We say that we can fit such a decision boundary using logistic regression with degree 2 polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize an arbitrary complex decision boundary?\n",
    "\n",
    "<img src=\"./fig/fig2.png\" style='height:300px;'>\n",
    "\n",
    "It's not easy to think of a function $g(x)$ can capture this decision boundary.\n",
    "\n",
    "**GOAL:** Find models that can capture *arbitrarily complex* functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximating Arbitrarily Complex Decision Boundaries\n",
    "\n",
    "Given an exact parametrization, we could learn the functional form, $g$, of the decision boundary directly. \n",
    "\n",
    "However, assuming an exact form for $g$ is restrictive. \n",
    "\n",
    "Rather, we can build increasingly good approximations, $\\widehat{g}$, of $g$ by composing simple functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Neural Network?\n",
    "\n",
    "**Goal:** build a good approximation $\\widehat{g}$ of a complex function $g$ by composing simple functions.\n",
    "\n",
    "For example, let the following picture represents $a = f\\left(\\sum_{i}w_ix_i\\right)$, where $f$ is a non-linear transform, and we denote the intermediate value $\\sum_{i}w_ix_i$ by $s$.\n",
    "\n",
    "<img src=\"./fig/fig4.png\" style='height:300px;'>\n",
    "\n",
    "**Note:** we always assume that $x_0=1$ and hence $w_0$ is the intercept or ***bias*** of the linear expression $\\sum_i w_i x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Translate Graphical Representations into Functional Ones\n",
    "\n",
    "Translate the following graphical representation of a function into a functional form, i.e. $g(x) = ?$ \n",
    "<img src=\"./fig/fig7.png\" style='height:150px;'>\n",
    "Use the following notation:\n",
    "\n",
    "**1.** the input node is $x$, the layer is 0<br>\n",
    "**2.** the weight from the $i$-th node in the $l$-th layer to the $j$-th node in the $l+1$-th layer is $w^l_{ij}$<br>\n",
    "**3.** the activation function $f(z) = e^{-0.5z^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks as Function Approximators\n",
    "\n",
    "Then we can define the approximation $\\widehat{g}$ with a graphical schema representing a complex series of compositions and sums of the form, $f\\left(\\sum_{i}w_ix_i\\right)$\n",
    "\n",
    "<img src=\"./fig/fig5.png\" style='height:300px;'>\n",
    "\n",
    "This is a ***neural network***. We denote the weights of the neural network collectively by $\\mathbf{W}$.\n",
    "The non-linear function $f$ is called the ***activation function***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Note:** \n",
    "Typically, at each node, we want to take a weighted sum of the values of the previous nodes with a additional ***bias term***. That is, we want as input $\\sum_i w^l_{ij} \\text{node}^l_i + \\text{bias}_j$ for the $j$-th node in the $l$-th hidden layer. This is often done by adding an extra node per layer with the value of 1:\n",
    "<img src=\"./fig/bias.png\" style='height:250px;'>\n",
    "The bias terms are considered to be part of the network parameters and when we jointly denote the network parameters by $\\mathbf{W}$, bias terms are included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Flexible Framework for Function Approximation\n",
    "\n",
    "\n",
    "<img src=\"./fig/fig6.png\" style='height:500px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common Choices for the Activation Function\n",
    "\n",
    "<img src=\"./fig/fig8.png\" style='height:500px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks are Universal Function Approximators\n",
    "\n",
    "So what kind of functions can be approximated by neural networks?\n",
    "\n",
    "**Theorem: (Hornik, Stinchombe, White, 1989)** Fix a \"nice\" activation function $f$. For any continuous function $g$ on a compact set $K$, there exists a feedforward neural network with activation $f$, having only a single hidden layer, which approximates $g$ to within an arbitrary degree of precision on $K$.\n",
    "\n",
    "For this reason, we call neural networks ***universal function approximators***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks Regression\n",
    "\n",
    "**Model for Regression:** $Y^{(n)}\\sim \\mathcal{N}(\\mu, \\sigma^2)$, $\\mu = g_\\mathbf{W}(\\mathbf{X}^{(n)})$, where $g_\\mathbf{W}$ is a neural network with parameters $\\mathbf{W}$.\n",
    "\n",
    "**Training Objective:** find $\\mathbf{W}$ to maximize the likelihood of our data. This is equivalent to minimizing the Mean Square Error,\n",
    "$$\n",
    "\\max_{\\mathbf{W}}\\, \\mathrm{MSE}(\\mathbf{W}) = \\frac{1}{N}\\sum^N_{n=1} \\left(y_n - g_\\mathbf{W}(x_n)\\right)^2\n",
    "$$\n",
    "\n",
    "**Optimizing the Training Objective:** For linear regression (when $g_\\mathbf{W}$ is a linear function), we computed the gradient of the MSE with respective to the model parameters $\\mathbf{W}$, set it equal to zero and solved for the optimal $\\mathbf{W}$ analytically (see Homework #0). For logistic regression, we computed the gradient and used (stochastic) gradient descent to \"solve for where the gradient is zero\".\n",
    "\n",
    "Can we do the same when $g_\\mathbf{W}$ is a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automatic Differentiation and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Computation for Neural Networks\n",
    "\n",
    "Computing the gradient for any parameter $w^l_{ij}$ in the following network requires us to use the ***chain rule***:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial t} g(h(t)) = g'(h(t))h'(t),\\quad& \\text{or}\\quad\\frac{\\partial g}{\\partial t} = \\frac{\\partial g}{\\partial h} \\frac{\\partial h}{\\partial t}\n",
    "\\end{align}\n",
    "\n",
    "This is because a neural network is just a big composition of functions.\n",
    "\n",
    "<img src=\"./fig/fig7.png\" style='height:150px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Computing Neural Network Gradients\n",
    "\n",
    "<img src=\"./fig/backprop.png\" style='height:600px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation: Gradient Descent for Neural Networks\n",
    "\n",
    "The ***backpropagation*** algorithm consists of three phases:\n",
    "0. (**Initialize**) intialize the network parameters $\\mathbf{W}$\n",
    "1. Repeat:\n",
    "  1. (**Forward Pass**) compute all intermediate values $s_{ij}^l$ and $a_{ij}^l$ for the given covariates $\\mathbf{X}$\n",
    "  2. (**Backward Pass**) compute all the gradients $\\frac{\\partial \\mathcal{L}}{\\partial w^l_{ij}}$\n",
    "  3. (**Update Parameters**) update each parameter by $-\\eta \\frac{\\partial \\mathcal{L}}{\\partial w^l_{ij}}$\n",
    "  \n",
    "<img src=\"./fig/graph_structure.png\" style='height:200px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Computation with Automatic Differentiation\n",
    "\n",
    "The forwards-backwards way of computing the gradient lends itself to an algorithm that automates gradient computation for any neural network. \n",
    "\n",
    "This is a special instance of ***reverse mode automatic differentiation*** -- a method of algorithmically computing exact gradients for functions defined by combinations of simple functions, by drawing graphical models of the composition of functions and then taking gradients by going forwards-backwards.\n",
    "<img src=\"./fig/function.png\" style='height:50px;'>\n",
    "\n",
    "<img src=\"./fig/computation_graph.png\" style='height:150px;'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# With Great Flexibility Comes with Great Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network Regression vs Linear Regression\n",
    "\n",
    "Linear models are easy to interpret. Once we've found the MLE of the model parameters, we can formulate scientific hypotheses about the relationship between the outcome $Y$ and the covariates $\\mathbf{X}$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\widehat{\\text{income}} = 2 * \\text{education (yr)} + 3.1 * \\text{married} - 1.5 * \\text{gaps in work history}\n",
    "\\end{align}\n",
    "\n",
    "What do the weights of a neural network tell you about the relationship between the covariates and the outcome?\n",
    "<img src=\"./fig/fig5.png\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization Error and Bias/Variance\n",
    "Complex models have ***low bias*** -- they can model a wide range of functions, given enough samples.\n",
    "\n",
    "But complex models like neural networks can use their 'extra' capacity to explain non-meaningful features of the training data that are unlikely to appear in the test data (i.e. noise). These models have ***high variance*** -- they are very sensitive to small changes in the data distribution, leading to drastic performance decrease from train to test settings.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"./fig/fig11.png\" style=\"width: 380px;\" align=\"center\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"./fig/fig12.png\" style=\"width: 380px;\" align=\"center\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
