{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #4: Sampling for Posterior Simulation\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "### Import basic libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Administrative Matters\n",
    "\n",
    "1. **Attendance Quiz:** <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "1. Review of Bayesian Modeling\n",
    "2. Connections to Frequentist Inference\n",
    "3. Basics of Sampling\n",
    "4. Inverse CDF Sampling\n",
    "5. Rejection Sampling\n",
    "6. Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Bayesian Modeling Process\n",
    "\n",
    "In order to make statements about $Y$, the outcome, and $\\theta$, parameters of the distribution generating the data, we form the joint distribution over both variables and use the various marginals/conditional distributions to reason about $Y$ and $\\theta$.\n",
    "\n",
    "1. we form the ***joint distribution*** over both variables $p(Y, \\theta) = p(Y | \\theta) p(\\theta)$.\n",
    "\n",
    "2. we can condition on the observed outcome to make inferences about $\\theta$,\n",
    "$$\n",
    "p(\\theta | Y) = \\frac{p(Y, \\theta)}{p(Y)}\n",
    "$$\n",
    "where $p(\\theta | Y)$ is called the ***posterior distribution*** and $p(Y)$ is called the ***marginal data likelihood***.\n",
    "3. before any data is observed, we can simulate data by using our prior\n",
    "$$\n",
    "p(Y^*) = \\int p(Y^*, \\theta) d\\theta = \\int p(Y^* | \\theta) p(\\theta) d\\theta\n",
    "$$\n",
    "where $Y^*$ represents new data and $p(Y^*)$ is called the ***prior predictive***.\n",
    "4. after observing data, we can simulate new data simliar to the observed data by using our posterior\n",
    "$$\n",
    "p(Y^*|Y) = \\int p(Y^*, \\theta|Y) d\\theta = \\int p(Y^* | \\theta) p(\\theta | Y) d\\theta\n",
    "$$\n",
    "where $Y^*$ represents new data and $p(Y^*|Y)$ is called the ***posterior predictive***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Empirical Estimation of Posterior and Prior Predictives\n",
    "\n",
    "As we have seen in the Beta-Binomial model, we can simulate the posterior (and prior) predictive rather than compute them analytically. That is, you don't need to know the pdf of $p(Y^*|Y)$. \n",
    "\n",
    "The posterior predictive can be represented by **samples** of predictions:\n",
    "\n",
    "1. we sample values of $\\theta_n$ from the posterior, $p(\\theta|Y)$.\n",
    "2. we sample an outcome $Y_n$ from $p(Y|\\theta_n)$ for each posterior sample $\\theta_n$.\n",
    "\n",
    "The set $Y_n$ we obtain emprirically represents the posterior predictve distribution $p(Y^*|Y)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Model for (Univariate) Gaussian Likelihood with Known Variance\n",
    "### The Bayesian Model\n",
    "\n",
    "Let $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$, with $\\sigma^2$ known. We place a normal prior on $\\mu$, $\\mu\\sim\\mathcal{N}(m, s^2)$. \n",
    "\n",
    "**Question:** is our choice of prior appropriate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Model for (Univariate) Gaussian Likelihood with Known Variance\n",
    "### Inference\n",
    "\n",
    "The posterior $p(\\mu|Y)$ is then:\n",
    "\\begin{aligned}\n",
    "p(\\mu | Y) = \\frac{p(Y| \\mu)p(\\mu)}{p(Y)} = \\frac{\\overbrace{\\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\mathrm{exp} \\left\\{-\\frac{(Y - \\mu)^2}{2\\sigma^2}\\right\\}}^{\\text{likelihood}} \\overbrace{\\frac{1}{\\sqrt{2\\pi s^2}} \\mathrm{exp} \\left\\{-\\frac{(m - \\mu)^2}{2s^2}\\right\\}}^{\\text{prior}}}{p(Y)}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "We can simplify the posterior as:\n",
    "\\begin{aligned}\n",
    "p(\\mu | Y) &= const *\\frac{\\mathrm{exp} \\left\\{ -\\frac{s^2(Y - \\mu)^2 + \\sigma^2(m - \\mu)^2}{2s^2\\sigma^2}\\right\\}}{p(Y)} \\\\\n",
    "&= const *\\mathrm{exp} \\left\\{ \\frac{s^2Y^2 + \\sigma^2m^2}{\\sigma^2 s^2}\\right\\}\\mathrm{exp} \\left\\{ -\\frac{(s^2 + \\sigma^2)\\mu^2 - 2(s^2Y + \\sigma^2m)\\mu}{2s^2\\sigma^2}\\right\\}\\\\\n",
    "&= const* \\mathrm{exp} \\left\\{ -\\frac{\\left(\\mu - \\frac{s^2Y + \\sigma^2m}{s^2 + \\sigma^2} \\right)^2}{2s^2\\sigma^2}\\right\\}\\quad \\text{(Completing the square)}\n",
    "\\end{aligned}\n",
    "Thus, we see that the posterior is a normal distribution, $\\mathcal{N}\\left(\\frac{s^2Y + \\sigma^2m}{s^2 + \\sigma^2}, s^2\\sigma^2\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Model for (Univariate) Gaussian Likelihood with Known Mean\n",
    "### The Bayesian Model\n",
    "\n",
    "Let $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$, with $\\mu$ known. We place an inverse-gamma prior on $\\sigma^2$, $\\sigma^2\\sim IG(\\alpha, \\beta)$. \n",
    "\n",
    "**Question:** is our choice of prior appropriate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Model for (Univariate) Gaussian Likelihood with Known Mean\n",
    "### Inference\n",
    "\n",
    "The posterior $p(\\sigma^2|Y)$ is then:\n",
    "\\begin{aligned}\n",
    "p(\\sigma^2 | Y) = \\frac{p(Y| \\sigma^2)p(\\sigma^2)}{p(Y)} = \\frac{\\overbrace{\\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\mathrm{exp} \\left\\{-\\frac{(Y - \\mu)^2}{2\\sigma^2}\\right\\}}^{\\text{likelihood}} \\overbrace{\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\left( \\sigma^2\\right)^{-\\alpha -1}\\mathrm{exp} \\left\\{-\\frac{\\beta}{\\sigma^2}\\right\\}}^{\\text{prior}}}{p(Y)}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can simplify the posterior as:\n",
    "\n",
    "\\begin{aligned}\n",
    "p(\\sigma^2 | Y) &= const * \\left( \\sigma^2\\right)^{-(\\alpha + 0.5) -1}\\mathrm{exp} \\left\\{-\\frac{\\frac{(Y-\\mu)^2}{2} + \\beta}{\\sigma^2}\\right\\}\n",
    "\\end{aligned}\n",
    "\n",
    "Thus, we see that the posterior is an inverse gamma distribution, $IG\\left(\\alpha + 0.5, \\frac{(Y-\\mu)^2}{2} + \\beta\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Model for (Univariate) Gaussian Likelihood with Unknown Mean and Variance\n",
    "\n",
    "Let $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$, with both parameters unknown. We place a normal prior on $\\mu$, $\\mu\\sim\\mathcal{N}(m, s^2)$, and an inverse-gamma prior on $\\sigma^2$, $\\sigma^2\\sim IG(\\alpha, \\beta)$.\n",
    "\n",
    "The posterior $p(\\sigma^2|Y)$ is then:\n",
    "\n",
    "\\begin{aligned}\n",
    "p(\\mu, \\sigma^2 | Y)  = \\frac{\\overbrace{\\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\mathrm{exp} \\left\\{-\\frac{(Y - \\mu)^2}{2\\sigma^2}\\right\\}}^{\\text{likelihood}} \\overbrace{\\frac{1}{\\sqrt{2\\pi s^2}} \\mathrm{exp} \\left\\{-\\frac{(m - \\mu)^2}{2s^2}\\right\\}}^{\\text{prior on $\\mu$}}\\overbrace{\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\left( \\sigma^2\\right)^{-\\alpha -1}\\mathrm{exp} \\left\\{-\\frac{\\beta}{\\sigma^2}\\right\\}}^{\\text{prior on $\\sigma^2$}}}{p(Y)}\n",
    "\\end{aligned}\n",
    "\n",
    "Can the posterior be simplified so that we recognize the form of the distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Bayesian Model for Poisson Likelihood\n",
    "### The Bayesian Model\n",
    "\n",
    "With the kidney cancer dataset in mind, let $Y\\sim Poi(N\\theta)$, where $N$ is the total population and $\\theta$ is the underlying cancer rate. We place a gamma prior on $\\theta$, $\\theta \\sim Ga(\\alpha, \\beta)$. \n",
    "\n",
    "**Question:** is our choice of prior appropriate?\n",
    "\n",
    "### Inference\n",
    "The posterior $p(\\theta|Y)$ is then:\n",
    "\\begin{aligned}\n",
    "p(\\theta | Y) = \\frac{p(Y| \\theta)p(\\theta)}{p(Y)} = \\frac{\\overbrace{\\frac{(N\\theta)^Y}{Y!} \\mathrm{exp} \\left\\{-N\\theta\\right\\}}^{\\text{likelihood}} \\overbrace{\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\left( \\theta\\right)^{\\alpha -1}\\mathrm{exp} \\left\\{-\\beta\\theta\\right\\}}^{\\text{prior}}}{p(Y)}\n",
    "\\end{aligned}\n",
    "\n",
    "We can simplify the posterior as:\n",
    "\n",
    "\\begin{aligned}\n",
    "p(\\theta | Y) &= const * \\theta^{\\alpha + Y - 1}\\mathrm{exp}\\left\\{-(N+ \\beta)\\theta \\right\\}\n",
    "\\end{aligned}\n",
    "\n",
    "Thus, we see that the posterior is a gamma distribution, $Ga\\left(\\alpha + Y, N + \\beta\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where do Priors Come From?\n",
    "\n",
    "Hopefully you've noticed a key property of the priors we chose:\n",
    "\n",
    ">  All the priors combined with the likelihood to form a distribution we recognize! Specifically, the posterior distribution is of the same type as the prior!\n",
    "\n",
    "These priors are called ***conjugate priors*** for the corresponding likelihoods. This is a purely mathematical property.\n",
    "\n",
    "**Question:** is it right to choose priors that are mathematically convenient? What is a good way to choose a prior? What if we \"choose wrong\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Uninformative Priors: How to Say I Don't Know\n",
    "\n",
    "If you don't have a strong prior believe about the parameters in the likelihood, you should choose a prior that has no effect when combined with the likelihood -- i.e. **let the data speak for itself**.\n",
    "\n",
    "**Example:**\n",
    "In the Beta-Binomial model, is there a choice of the hyperparameters of the prior that has no effect on the posterior?\n",
    "\n",
    "We might try $Beta(1, 1)$, which is the uniform distribution over $[0, 1]$. The posterior $p(\\theta|Y)$ is then $Beta(Y + 1, N - Y + 1)$. In the following example, visualizing the posterior, we see that the posterior assigns the highest mass to the MLE of $\\theta$. But the posterior assigns more mass to values of $\\theta$ smaller than the MLE. In this sense, the prior **does** inform the posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y = 11\n",
    "N = 15\n",
    "MLE = Y * 1. / N\n",
    "posterior = sp.stats.beta(Y + 1, N - Y + 1).pdf\n",
    "posterior_mean = (Y + 1.) / (N + 2.)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "x = np.linspace(0, 1, 100)\n",
    "ax.plot(x, posterior(x), color='red', label='Posterior pdf')\n",
    "ax.axvline(x=MLE, color='black', label='Maximum Likelihood Estimate')\n",
    "ax.axvline(x=posterior_mean, color='green', label='Posterior Mean')\n",
    "ax.set_title('Comparison of the posterior with the MLE')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There is a general purpose technique for creating non-informative priors, called ***Jeffreys priors***, which places less prior weight on parameter values where the likelihood function is flat. This prevents the prior from having undue influency on the posterior.\n",
    "\n",
    "There are many resources that cover the construction of Jeffreys priors for common likelihoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Connections with Frequentist Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point Estimates from the Posterior\n",
    "If you absolutely wanted to derive a point estimate for the parameters $\\theta$ in the likelihood from your Bayesian model, there are two common ways to do it:\n",
    "\n",
    "1. ***the posterior mean***\n",
    "$$\n",
    "\\theta_{\\text{post mean}} = \\mathbb{E}_{\\theta\\sim p(\\theta|Y)}\\left[ \\theta|Y \\right] = \\int \\theta p(\\theta|Y) d\\theta\n",
    "$$\n",
    "2. ***the posterior mode*** or ***maximum a posterior (MAP)*** estimate\n",
    "$$\n",
    "\\theta_{\\text{MAP}} = \\mathrm{argmax}_{\\theta} p(\\theta|Y)\n",
    "$$\n",
    "\n",
    "**Question:** is it better to summarize the entire posterior using a point estimate? I.e. why should we keep the posterior distribution around?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point Estimates Can Be Misleading\n",
    "\n",
    "The posterior mode can be an atypical point: \n",
    "<img src=\"fig/map.jpg\" style=\"height:250px; width:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point Estimates Can Be Misleading\n",
    "\n",
    "The posterior mean can be an unlikely point: \n",
    "\n",
    "<img src=\"fig/mean.jpg\" style=\"height:250px; width:450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparison of Posterior Point Estimates and MLE\n",
    "\n",
    "1. **Beta-Binomial Model for Coin Flips**\n",
    " - Likelihood: $Bin(N, \\theta)$ \n",
    " - Prior: $Beta(\\alpha, \\beta)$\n",
    " - MLE: $\\frac{Y}{N}$\n",
    " - MAP: $\\frac{Y + \\alpha - 1}{N + \\alpha + \\beta - 2}$\n",
    " - Posterior Mean: $\\frac{Y + \\alpha}{N + \\alpha + \\beta}$ \n",
    " \n",
    " \n",
    " 2. **Poisson-Gamma Model for Cancer Rates**\n",
    "  - Likelihood: $Poi(N\\theta)$ \n",
    "  - Prior: $Ga(\\alpha, \\beta)$\n",
    "  - MLE: $\\frac{Y}{N}$\n",
    "  - MAP: $\\frac{Y + \\alpha - 1}{N + \\beta}$\n",
    "  - Posterior Mean: $\\frac{Y + \\alpha}{N + \\beta}$ \n",
    "\n",
    "**Question:** What is the effect of the prior on the posterior point estimates? Imagine if $Y=10$, $N=11$, $\\alpha=100$, $\\beta=300$. What if $Y=1,000$, $N=11,000$, $\\alpha=1$, $\\beta=3$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Coin Toss Example: Revisited Yet Again\n",
    "Recall that one way to prevent the MLE from overfitting is to add ***regularization terms***:\n",
    "$$\n",
    "\\theta_{\\text{MLE Reg}} = \\frac{Y + \\alpha}{N + \\beta}.\n",
    "$$\n",
    "This is very similar to the MAP and posterior mean estimates:\n",
    " - MAP: $\\frac{Y + \\alpha - 1}{N + \\alpha + \\beta - 2}$\n",
    " - Posterior Mean: $\\frac{Y + \\alpha}{N + \\alpha + \\beta}$ \n",
    "\n",
    "In fact, we have see that one effect of adding a prior is that it **regularizes** our inference about $\\theta$.\n",
    "\n",
    "**Question:** What happens to the MAP and posterior mean estimates as $N$ (and hence $Y$) becomes very large?\n",
    "\n",
    "$$\n",
    "\\lim_{N\\to \\infty} \\frac{Y + \\alpha - 1}{N + \\alpha + \\beta - 2} = ?\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Law of Large Numbers for Bayesian Inference\n",
    "\n",
    "In general, in Bayesian inference we are **less interested asymptotic behavior**. But the properties of the asymptotic distribution of the posterior can be useful.\n",
    "\n",
    "**Theorem: (Berstein-von Mises**)\n",
    "\n",
    "*\"Under some conditions, as $N\\to \\infty$ the posterior distribution converges to a Gaussian distribution centred at the MLE with covariance matrix given by a function of the Fisher information matrix at the true population parameter value.\"*\n",
    "\n",
    "**Consequences**\n",
    "1. The posterior point estimates approach the MLE, with large samples sizes. <br><br>\n",
    "2. It may be valid to approximate the posterior with a Gaussian, with large samples sizes. This will become a very important idea during the second half of the course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computational Comparisons\n",
    "\n",
    "1. **Computation of the MLE is an optimization problem.** Although difficult, there are many established methods for performing optimization (even when the objective function is not convex -- i.e. many local optima). \n",
    "\n",
    "  More importantly, there are algorithms to perform general, automatic optimization (e.g. gradient descent) on a large class of functions -- that is, we do not need to artisanally solve an optimization problem for each statistical model.<br><br>\n",
    "  \n",
    "2. **Computation of the posterior (thus far) is an process of choosing the right priors and noting that the posterior distribution is of the same type as the prior.** The derivation is simple so long as we use conjugate priors. But many intuitively appropriate priors (like the inverse gamma and normal priors for a univariate gaussian) are not conjugate. In those cases, it becomes intractable to\n",
    " - compute posterior mode or mean\n",
    " - simulate samples from the posterior (and hence simulate samples from the posterior predictive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basics of Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Sampling?\n",
    "\n",
    "Given a distribution $p(x)$ over a space $\\mathbb{R}^D$, ***sampling*** $x \\sim p(x)$ means to generate a random $x\\in \\mathbb{R}^D$, such that the asymptotic frequencies of the samples generated is described by the pdf $p(x)$.\n",
    "\n",
    "A ***sampler*** is an algorithm or procedure that produces numbers with a certain distribution $p(x)$.\n",
    "\n",
    "In practice, \"random numbers\" are simulated using deterministic algorithms called ***pseudo number generators***. A pseudo number generator takes an initial value, the ***random seed***, and produces an array of random looking numbers (from a uniform distribution).\n",
    "\n",
    "**Note:** for a given pseudo number generator, if the random seed is fixed then the random number output will also be fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simulating a Uniform Random Variable: Linear Congruence\n",
    "\n",
    "Fix an integer $c > N$ and fix integers $a, b > 0$. \n",
    "1. seed: set an integer $0\\leq s_0 < c$\n",
    "2. iterate $N$ times: $s_n = (as_{n-1} + b)_{\\text{mod } c}$\n",
    "\n",
    "Output is an array of random integers $[s_0, \\ldots, s_N]$ in $[0, c]$. \n",
    "\n",
    "For an array of random real numbers in $[0, 1]$, we compute $\\left[\\frac{s_0}{c}, \\ldots, \\frac{s_N}{c}\\right]$. \n",
    "\n",
    "We'll see that **the simulation of all random variables is based on the simulation of the uniform distribution over $[0, 1]$**!\n",
    "\n",
    "**Note:** the apparent randomness of the output is sensitive to choices of $a, b, c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.38, 0.23, 0.58, 0.43, 0.78, 0.63, 0.98, 0.83, 0.18, 0.03])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parameters of linear congruence algorithm\n",
    "c = 100\n",
    "a = 11\n",
    "b = 5\n",
    "#total number of simulations\n",
    "N = 10\n",
    "#random seed\n",
    "s_current = 3\n",
    "#array of random numbers\n",
    "random_numbers = []\n",
    "\n",
    "#run the linear congruence algorithm N times\n",
    "for n in range(N):\n",
    "    s_next = (a * s_current + b) % c\n",
    "    random_numbers.append(s_next)\n",
    "    s_current = s_next\n",
    "    \n",
    "#convert random integers to random real values in [0, 1]    \n",
    "random_numbers = np.array(random_numbers) * 1.\n",
    "random_numbers /= c\n",
    "#print\n",
    "random_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inverse CDF Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Cumulative Distribution Function\n",
    "\n",
    "Recall that the ***cumulative distribution function (CDF)*** of a real-valued random variable $X$ with continuous pdf $f_X$ is defined as\n",
    "$$\n",
    "F_X(a) = \\mathbb{P}[X \\leq a] = \\int_{-\\infty}^a f_X(t) dt\n",
    "$$\n",
    "<img src=\"fig/cdf.jpg\" style=\"height:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverse CDF Sampling: An Intuition\n",
    "The idea behind Inverse CDF Sampling is that while it is sometimes difficult to generate values for $X$ with the relative frequency described by pdf, $f_X$, it can be easier to generate values for $X$ using the CDF. \n",
    "\n",
    "The intuition is as follows:\n",
    "\n",
    "1. While the support of the pdf and CDF can be unbounded, the range of the CDF is bounded between 0 and 1.<br><br>\n",
    "2. The CDF for a continuous single-variable pdf is an invertible function (on the support of the pdf). That is, each value between 0 and 1 corresponds to a unique value of the random variable $X$.<br><br>\n",
    "3. Values of $X$ that lie under peaks of the pdf occupy larger portions of the interval [0, 1]. That is, the range of the CDF, [0, 1], can be subdivided to exactly reflect the areas of high probability mass and low probability mass under the pdf. \n",
    "\n",
    "So if we uniformly sample values in the range of the CDF, [0, 1] and find the corresponding $X$ values for these samples (using the inverse function of the CDF), we obtain samples of $X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverse CDF Sampling: An Intuition\n",
    "<img src=\"fig/inverse_cdf.jpg\" style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inverse CDF Sampling: Algorithm\n",
    "\n",
    "We use a random variable $U$ with uniform distribution over $[0, 1]$ to simulate a univariate random variable $X$ with pdf $f_X$, where:\n",
    "- we know the analytical form of the CDF of $X$, $F_X$.\n",
    "- we know the analytical form of the inverse of the CDF of $X$, $F^{-1}_X$.\n",
    "\n",
    "To simulate $X$, we repeat for $N$ number of samples:\n",
    "1. sample $U_n \\sim U(0, 1)$\n",
    "2. compute $X_n = F^{-1}_X(U_n)$\n",
    "\n",
    "**Question:** How do we simulate multivariate random variables? What if we don't have the analytical form of $F^{-1}_X$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Inverse CDF Sampling: Proof of Correctness\n",
    "\n",
    "**Theorem:**  Let $F_X$ be the continuous and strictly increasing CDF of a real-valued random variable $X$. Let $F_X^{1} : [0,1] \\to \\mathbb{R}$ be the inverse function of $F_X$. Let $U \\sim U(0,1)$, then the random variable $Y = F_X^{1}(U)$ has CDF $F_X$.\n",
    "\n",
    "***Proof:*** Recall that the CDF of $Y$ is defined by $F_Y(x) = \\mathbb{P}[Y \\leq x]$. Now, we can write\n",
    "\n",
    "\\begin{aligned}\n",
    "F_Y(x) &= \\mathbb{P}[Y \\leq x]&\\\\ \n",
    "&= \\mathbb{P}[F_X^{1}(U) \\leq x],&(\\text{since $Y = F_X(U)$})\\\\\n",
    "&= \\mathbb{P}[F_X(F_X^{1}(U)) \\leq F_X(x)],&(\\text{since $F_X$ is strictly increasing})\\\\\n",
    "&= \\mathbb{P}[U \\leq F_X(x)],&(\\text{since $F_X^{-1}$ is the inverse of $F_X$})\\\\\n",
    "&= F_X(x)& (\\text{since $0\\leq F_X\\leq 1$ and by the properties of uniform RV's})\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simulating an Exponential Random Variable\n",
    "We'll use a uniform random variable $U\\sim U(0, 1)$ to simulate a exponential variable $X\\sim Exp(\\lambda)$. Recall that the exponential CDF is\n",
    "$$\n",
    "F_X(x) = 1 - e^{-\\lambda x}.\n",
    "$$\n",
    "\n",
    "The inverse of the CDF can be found by solving for the input, $x$, of the CDF\n",
    "\\begin{aligned}\n",
    "y &= 1 - e^{-\\lambda x}\\\\\n",
    "e^{-\\lambda x} &= 1 - y\\\\\n",
    "-\\lambda x &= \\log(1 - y)\\\\\n",
    "x &= -\\frac{1}{\\lambda}\\log(1 - y)\n",
    "\\end{aligned}\n",
    "where we take $\\log$ to be base $e$.\n",
    "\n",
    "Thus, $F_X^{-1}(y) = -\\frac{1}{\\lambda}\\log(1 - y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "N = 500\n",
    "lam = 5\n",
    "#define x values that include most of the probability mass of the pmf\n",
    "x = np.arange(sp.stats.poisson.ppf(0.01, mu), sp.stats.poisson.ppf(0.99, mu))\n",
    "#get the poisson pmf function from scipy.stats\n",
    "poisson_pmf = sp.stats.poisson(mu).pmf\n",
    "inverse_cdf = lambda y: -1. / lam * np.log(1 -  y)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 2))\n",
    "ax.plot\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Simulating a Bernoulli Random Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Simulating a Poisson Random Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rejection Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rejection Sampling: An Intuition\n",
    "The idea behind Rejection Sampling is to by-pass the problem of sampling from a difficult distribution $f_X$, by:\n",
    "\n",
    "1. approximating $f_X$ (called the ***target distribution***), with a pdf $g$ (called the ***proposal distribution***) that is easy to sample\n",
    "2. sample from $g$ and reject the samples that are unlikely to be from $f_X$\n",
    "\n",
    "\n",
    "<img src=\"fig/rejection.jpg\" style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rejection Sampling: Algorithm\n",
    "\n",
    "We can use rejection sampling to simulate multivariate random variables and random variables for which we don't have a closed form for $F^{-1}_X$. We choose a proposal distribution $g$ and a non-zero constant $M$ such that $f_X \\leq M\\cdot g$. \n",
    "\n",
    "To simulate $X$, we repeat until $N$ samples are accepted:\n",
    "1. sample $X_k \\sim g(X)$\n",
    "2. sample a random height, $U_k \\sim U(0, 1)$\n",
    "3. if $U_k < \\frac{f_X(X_k)}{Mg(X_k)}$ then accept $X_k$ as a sample, else reject\n",
    "\n",
    "**Question:** How long does it take to accumulate $N$ samples? What is the effect of the choice of the proposal distribution $g$ have on the sampling process? Now that we can sample from any distribution, this class is over right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Rejection Sampling: Proof of Correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rejection Sampling: Efficiency\n",
    "\n",
    "A sample $X = x$ generated from $g$ is accepted with probability $\\frac{f_X}{M g}$. One can compute that the expected number of times it takes to draw and accept a sample $X=x$ is precisely $M$. This means that roughly $(M-1)/M$ of samples drawn from $g$ will be rejected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simulating a Normal Random Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Semi-Conjugate Priors\n",
    "\n",
    "Let $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$, with both parameters unknown. We place a normal prior on $\\mu$, $\\mu\\sim\\mathcal{N}(m, s^2)$, and an gamma prior on $\\sigma^2$, $\\sigma^2\\sim IG(\\alpha, \\beta)$.\n",
    "\n",
    "The posterior $p(\\mu, \\sigma^2|Y)$ is then:\n",
    "\n",
    "\\begin{aligned}\n",
    "p(\\mu, \\sigma^2 | Y)  = \\frac{\\overbrace{\\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\mathrm{exp} \\left\\{-\\frac{(Y - \\mu)^2}{2\\sigma^2}\\right\\}}^{\\text{likelihood}} \\overbrace{\\frac{1}{\\sqrt{2\\pi s^2}} \\mathrm{exp} \\left\\{-\\frac{(m - \\mu)^2}{2s^2}\\right\\}}^{\\text{prior on $\\mu$}}\\overbrace{\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\left( \\sigma^2\\right)^{-\\alpha -1}\\mathrm{exp} \\left\\{-\\frac{\\beta}{\\sigma^2}\\right\\}}^{\\text{prior on $\\sigma^2$}}}{p(Y)}\n",
    "\\end{aligned}\n",
    "\n",
    "Note that:\n",
    "\n",
    "1. if we condition on $\\sigma^2$ (i.e. hold it constant) then $p(\\mu| Y, \\sigma^2)$ is a normal pdf, $\\mathcal{N}\\left(\\mu; \\frac{s^2y + \\sigma^2m}{s^2 + \\sigma^2}, s^2\\sigma^2\\right)$.\n",
    "2. if we condition on $\\mu$ (i.e. hold it constant) then $p(\\sigma^2| Y, \\mu)$ is an inverse gamma pdf, $IG\\left(\\alpha + 0.5, \\frac{(y-\\mu)^2}{2} + \\beta\\right)$\n",
    "\n",
    "That is, **the conditional of the posterior are easy to sample from while the joint posterior is not**. In this case, we call the priors ***semi-conjugate*** for our likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gibbs Sampling: An Intuition\n",
    "If we start at a point $(x^{(0)}, y^{(0)})$ sampled from from the joint distribution $p(X, Y)$, we can get to the next point $(x^{(1)}, y^{(1)}) \\sim p(X, Y)$ through a \"stepping-stone\" $(x^{(1)}, y^{(0)})$, where we updated the first coordinate by $x^{(1)} \\sim p(X|Y = y^{(0)})$. From there, we update the second coordinate $y^{(1)} \\sim p(Y|X = x^{(1)})$.\n",
    "<img src=\"fig/gibbs.jpg\" style=\"height:350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gibbs Sampling: Algorithm\n",
    "\n",
    "To simulate $N$ samples of a $D$-dimensional multivariate random variable $X$ with pdf $f_X$, we\n",
    "1. initialization: choose any $x^{(0)} = \\left[x^{(0)}_1\\;\\; \\ldots\\;\\; x^{(0)}_D\\right]$\n",
    "2. iterate $N$ times: sample $x^{(n+1)} = \\left[x^{(n+1)}_1\\;\\; \\ldots\\;\\; x^{(n+1)}_D\\right]$ by\n",
    "\n",
    "  a. initialization: sample $X^{(n+1)}_1$ from the conditional distribution \n",
    "  \n",
    "  $$f_X\\left(X_1 \\,|\\, X_2 = x^{(n)}_2,\\;\\ldots,\\; X_D = x^{(n)}_D\\right)$$\n",
    "  \n",
    "  b. iterate from $d = 2$ througbh $d=D$: sample $x^{(n+1)}_d$ from the conditional distribution\n",
    "  \n",
    "  $$f_X\\left(X_d \\,|\\, X_1 = x^{(n+1)}_1,\\; \\ldots,\\; X_{d-1} = x^{(n+1)}_{d-1},\\; X_{d+1} = x^{(n)}_{d+1},\\; \\ldots,\\; X_D = x^{(n)}_D\\right)$$\n",
    "  \n",
    "**Question:** Why is this algorithm a valid sampler? That is, how do we prove that the samples we obtain are actually distributed as $f_X$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simulating the Posterior of a Normal-Normal-Inverse Gamma Model"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
